{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bccb65e-f57b-49f3-a946-e11654216f3a",
   "metadata": {},
   "source": [
    "# Removing Features or Removing Rows\n",
    "\n",
    "If only a few rows relative to the size of your dataset are missing some values, then it might just be a good idea to drop those rows. What does this cost you in terms of performace? It essentialy removes potential training/testing data, but if its only a few rows, its unlikely to change performance.\n",
    "\n",
    "Sometimes it is a good idea to remove a feature entirely if it has too many null values. However, you should carefully consider why it has so many null values, in certain situations null could just be used as a separate category. \n",
    "\n",
    "Take for example a feature column for the number of cars that can fit into a garage. Perhaps if there is no garage then there is a null value, instead of a zero. It probably makes more sense to quickly fill the null values in this case with a zero instead of a null. Only you can decide based off your domain expertise and knowledge of the data set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9245af29",
   "metadata": {},
   "source": [
    "## Working based on Rows Missing Data\n",
    "\n",
    "## Filling in Data or Dropping Data?\n",
    "\n",
    "Let's explore how to choose to remove or fill in missing data for rows that are missing some data. Let's choose some threshold where we decide it is ok to drop a row if its missing some data (instead of attempting to fill in that missing data point). We will choose 1% as our threshold. This means if less than 1% of the rows are missing this feature, we will consider just dropping that row, instead of dealing with the feature itself. There is no right answer here, just use common sense and your domain knowledge of the dataset, obviously you don't want to drop a very high threshold like 50% , you should also explore correlation to the dataset, maybe it makes sense to drop the feature instead.\n",
    "\n",
    "Based on the text description of the features, you will see that most of this missing data is actually NaN on purpose as a placeholder for 0 or \"none\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04195ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with missing values\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Dropping columns with missing values\n",
    "df.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73083803",
   "metadata": {},
   "source": [
    "### 2-Mean/median imputation:\n",
    "#### Replace missing values with the mean or median value of the feature. This strategy works well if the missing values are numerical and the distribution is roughly symmetric.\n",
    "Example code in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9879fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values with mean\n",
    "mean = df['column_name'].mean()\n",
    "df['column_name'].fillna(mean, inplace=True)\n",
    "\n",
    "# Imputing missing values with median\n",
    "median = df['column_name'].median()\n",
    "df['column_name'].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642c2530",
   "metadata": {},
   "source": [
    "### 3-Mode imputation:\n",
    "#### Replace missing values with the most frequent value of the feature. This strategy works well if the missing values are categorical.\n",
    "Example code in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2542d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values with mode\n",
    "mode = df['column_name'].mode()[0]\n",
    "df['column_name'].fillna(mode, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9d8d7",
   "metadata": {},
   "source": [
    "#### 4-Regression imputation:\n",
    "### Use regression models to predict the missing values based on other features in the dataset. This strategy works well when the missing values are numerical and there is a strong correlation with other features.\n",
    "Example code in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f318306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Splitting dataset into two parts - with and without missing values\n",
    "known = df[df['column_name'].notna()]\n",
    "unknown = df[df['column_name'].isna()]\n",
    "\n",
    "# Training regression model on known values\n",
    "X_train = known.drop('column_name', axis=1)\n",
    "y_train = known['column_name']\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predicting missing values using regression model\n",
    "X_test = unknown.drop('column_name', axis=1)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Filling in missing values with predicted values\n",
    "df.loc[df['column_name'].isna(), 'column_name'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798c661",
   "metadata": {},
   "source": [
    "### 5-Multiple imputation:\n",
    "#### Use statistical models to generate multiple imputed datasets and combine the results. This strategy works well when there are complex patterns of missingness in the dataset.\n",
    "Example code in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3717e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Creating multiple imputed datasets\n",
    "imp = IterativeImputer(random_state=0)\n",
    "imputed = imp.fit_transform(df)\n",
    "\n",
    "# Combining results from multiple imputed datasets\n",
    "df = pd.DataFrame(imputed, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac79ae",
   "metadata": {},
   "source": [
    "### 6-Forward fill/backward fill:\n",
    "#### Replace missing values with the previous or next valid value along the feature axis. This strategy works well when the missing values occur in a time series or sequential data\n",
    "Example code in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward filling missing values\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Backward filling missing values\n",
    "df.fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db2996",
   "metadata": {},
   "source": [
    "### 1-Drop rows/columns with missing values:\n",
    "#### This strategy involves removing rows or columns which have missing values. This can be done when the amount of missing data is small and doesn't significantly affect the dataset's overall quality. However, this strategy can lead to loss of valuable information and reduce the sample size.\n",
    "Example in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d5357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name   age gender\n",
      "0  Alice  25.0      F\n",
      "1    Bob  32.0      M\n",
      "3  David  28.0      M\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset with missing values\n",
    "data = {'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "        'age': [25, 32, None, 28, 30],\n",
    "        'gender': ['F', 'M', 'M', 'M', None]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "print(df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e051b79f",
   "metadata": {},
   "source": [
    "### 2-Imputation:\n",
    "#### Imputation involves filling in the missing values with a reasonable estimate. This can be done using various methods, such as mean, median, mode, or using machine learning algorithms to predict the missing values.\n",
    "Example in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values with mean\n",
    "mean = df['column_name'].mean()\n",
    "df['column_name'].fillna(mean, inplace=True)\n",
    "\n",
    "# Imputing missing values with median\n",
    "median = df['column_name'].median()\n",
    "df['column_name'].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c00fd8-1eea-498e-b157-b007cf4acbfc",
   "metadata": {},
   "source": [
    "# **SimpleImputer**\n",
    "## sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcbba74a-622e-41ab-9286-d0127ececc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name    age gender\n",
      "0    Alice  25.00      F\n",
      "1      Bob  32.00      M\n",
      "2  Charlie  28.75      M\n",
      "3    David  28.00      M\n",
      "4      Eva  30.00   None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create the DataFrame\n",
    "data = {'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "        'age': [25, 32, None, 28, 30],\n",
    "        'gender': ['F', 'M', 'M', 'M', None]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the SimpleImputer with strategy='mean'\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the 'age' column\n",
    "df['age'] = imputer.fit_transform(df[['age']])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc284a9-5846-4e5b-acd9-d253832ae803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      name   age   tall  weight\n",
      "0    Alice  25.0  160.0    55.0\n",
      "1      Bob  32.0    NaN    70.0\n",
      "2  Charlie   NaN  175.0     NaN\n",
      "3    David  28.0  180.0    80.0\n",
      "4      Eva  30.0  165.0    65.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create the DataFrame\n",
    "data = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "    'age': [25, 32, None, 28, 30],\n",
    "    'tall': [160, None, 175, 180, 165],\n",
    "    'weight': [55, 70, None, 80, 65]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ceab0d3-91cc-4eb7-b113-14347361189b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns: Index(['age', 'tall', 'weight'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Select numerical columns\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "print(\"Numerical Columns:\", numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e77d86-89e1-4544-8687-30f163a8199b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame After Imputation:\n",
      "      name   age   tall  weight\n",
      "0    Alice  25.0  160.0    55.0\n",
      "1      Bob  32.0  170.0    70.0\n",
      "2  Charlie  29.0  175.0    67.5\n",
      "3    David  28.0  180.0    80.0\n",
      "4      Eva  30.0  165.0    65.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Initialize the SimpleImputer with strategy='mean'\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Apply the imputer to the numerical columns\n",
    "df[numerical_columns] = imputer.fit_transform(df[numerical_columns])\n",
    "\n",
    "print(\"\\nDataFrame After Imputation:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952b5ec",
   "metadata": {},
   "source": [
    "### 3-Categorical imputation:\n",
    "#### This strategy involves filling in missing values for categorical variables using the most frequent category.\n",
    "Example in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e273e531-c86c-4deb-a19d-8a3c5bdc5b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name   age gender\n",
      "0    Alice  25.0      F\n",
      "1      Bob  32.0      M\n",
      "2  Charlie   NaN      M\n",
      "3    David  28.0      M\n",
      "4      Eva  30.0      M\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame\n",
    "data = {'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "        'age': [25, 32, None, 28, 30],\n",
    "        'gender': ['F', 'M', 'M', 'M', None]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Replace None with np.nan in the 'gender' column\n",
    "df['gender'] = df['gender'].replace([None], np.nan)\n",
    "\n",
    "# Initialize the SimpleImputer with strategy='most_frequent' for the 'gender' column\n",
    "imputer_gender = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Apply the imputer to the 'gender' column and flatten the result\n",
    "df['gender'] = imputer_gender.fit_transform(df[['gender']]).flatten()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21089073",
   "metadata": {},
   "source": [
    "## 4-Interpolation:\n",
    "### This strategy involves filling in missing values using a linear or polynomial function that estimates the values between two known points. This method is useful when the data is continuous and the missing data is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d4d66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name   age gender\n",
      "0    Alice  25.0      F\n",
      "1      Bob  32.0      M\n",
      "2  Charlie  30.0      M\n",
      "3    David  28.0      M\n",
      "4      Eva  30.0   None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset with missing values\n",
    "data = {'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "        'age': [25, 32, None, 28, 30],\n",
    "        'gender': ['F', 'M', 'M', 'M', None]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Interpolate missing values using linear interpolation\n",
    "df_interpolated = df.interpolate()\n",
    "print(df_interpolated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f8ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b1d08-556c-484a-8a1a-0b3bca081b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35042ce-0514-4dd4-ac96-011c2b23eb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
